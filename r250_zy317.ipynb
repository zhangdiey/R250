{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import torch\n",
    "import spacy\n",
    "import random\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "from nltk.metrics import *\n",
    "\n",
    "random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# read data\n",
    "# run once\n",
    "# with open('data/train.txt','r') as source:\n",
    "#     data = [ (random.random(), line) for line in source ]\n",
    "# data.sort()\n",
    "# data = [line for _, line in data]\n",
    "\n",
    "# data_size = len(data)\n",
    "# train_data = data[:int(data_size * 0.9)]\n",
    "# val_data = data[int(data_size * 0.9):]\n",
    "\n",
    "# with open('data/test.txt','r') as source:\n",
    "#     test_data = source.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# preprocess data into train, val, and test set\n",
    "# run once\n",
    "# for src_path, tgt_path, dataset in zip(['src_train.txt', 'src_val.txt', 'src_test.txt'], \\\n",
    "#                                        ['tgt_train.txt', 'tgt_val.txt', 'tgt_test.txt'], \\\n",
    "#                                        [train_data, val_data, test_data]):\n",
    "#     with open('data/' + src_path,'w+') as src:\n",
    "#         with open('data/' + tgt_path,'w+') as tgt:\n",
    "#             for line in dataset:\n",
    "#                 cur_file = src\n",
    "#                 for tok in line.strip().split(sep=' '):\n",
    "#                     if tok == 'IN:':\n",
    "#                         continue\n",
    "#                     elif tok == \"OUT:\":\n",
    "#                         cur_file.write('\\n')\n",
    "#                         cur_file = tgt\n",
    "#                     else:\n",
    "#                         cur_file.write(tok+' ')\n",
    "#                 cur_file.write('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation methods\n",
    "def eval(path_to_gold, path_to_pred):\n",
    "    candidates = []\n",
    "    references = []\n",
    "\n",
    "    with open(path_to_gold, 'r') as gold_f:\n",
    "        gold = gold_f.readlines()\n",
    "        gold_size = len(gold)\n",
    "        with open(path_to_pred, 'r') as pred_f:\n",
    "            pred = pred_f.readlines()\n",
    "            pred_size = len(pred)\n",
    "            if gold_size != pred_size:\n",
    "                Exception()\n",
    "            sent_tp = 0\n",
    "            c_precison = 0\n",
    "            c_recall = 0\n",
    "            c_f = 0\n",
    "            tp = 0\n",
    "            fp = 0\n",
    "            tn = 0\n",
    "            fn = 0\n",
    "            for i, gold_line in enumerate(gold):\n",
    "                pred_line = pred[i].strip()\n",
    "                gold_line = gold_line.strip()\n",
    "                pred_set = set(pred_line.split())\n",
    "                gold_set = set(gold_line.split())\n",
    "                candidates.append(pred_line)\n",
    "                references.append([gold_line])\n",
    "                if 'I_JUMP' in gold_set:\n",
    "                    if 'I_JUMP' in pred_set:\n",
    "                        tp += 1\n",
    "                    else:\n",
    "                        fn += 1\n",
    "                else:\n",
    "                    if 'I_JUMP' in pred_set:\n",
    "                        fp += 1\n",
    "                    else:\n",
    "                        tn += 1\n",
    "\n",
    "                if precision(pred_set, gold_set):\n",
    "                    c_precison += precision(pred_set, gold_set)\n",
    "                if recall(pred_set, gold_set):\n",
    "                    c_recall += recall(pred_set, gold_set)\n",
    "                if f_measure(pred_set, gold_set):\n",
    "                    c_f += f_measure(pred_set, gold_set)\n",
    "                if gold_line == pred_line:\n",
    "                    sent_tp += 1\n",
    "\n",
    "    if tp+fp:\n",
    "        p = tp / (tp+fp)\n",
    "    else:\n",
    "        p = 0.0\n",
    "    if tp+fn:\n",
    "        r = tp / (tp+fn)\n",
    "    else:\n",
    "        r = 0.0\n",
    "    if p+r:\n",
    "        f1 = 2*r*p/(r+p)\n",
    "    else:\n",
    "        f1 = 0.0\n",
    "\n",
    "    print('Sent-Level Acc', sent_tp/gold_size)\n",
    "    print('Average Precision', c_precison/gold_size)\n",
    "    print('Average Recall', c_recall/gold_size)\n",
    "    print('Average F1', c_f/gold_size)\n",
    "    print('New Primitive Precision', p)\n",
    "    print('New Primitive Recall', r)\n",
    "    print('New Primitive F1', f1)\n",
    "    print('BLEU', corpus_bleu(references, candidates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build baseline vocab (before data augmentation)\n",
    "# run once\n",
    "# !onmt_build_vocab -config vocab_base.yaml -n_sample -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# baseline training\n",
    "# run once\n",
    "# !onmt_train -config train_base.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# baseline prediction\n",
    "# run once\n",
    "# !onmt_translate -model models/base.pt -src data/src_test.txt -output pred/base.txt -gpu 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate baseline model\n",
    "# eval('data/tgt_test.txt', 'pred/base.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# with open('data/src_train.txt','r') as f:\n",
    "#     src_train = f.readlines()\n",
    "# with open('data/tgt_train.txt','r') as f:\n",
    "#     tgt_train = f.readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1-replace data augmentation\n",
    "# run once\n",
    "# with open('data/src_train_aug_1.txt', 'w+') as src:\n",
    "#     with open('data/tgt_train_aug_1.txt','w+') as tgt:\n",
    "#         for src_1, tgt_1 in list(zip(src_train, tgt_train)):\n",
    "#             if len(src_1.split()) > 1:\n",
    "#                 src_1_aug = src_1.replace('run','jump')\n",
    "#                 tgt_1_aug = tgt_1.replace('I_RUN','I_JUMP')\n",
    "#                 src.write(f'{src_1_aug}')\n",
    "#                 tgt.write(f'{tgt_1_aug}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build vocab (use 1-replace data augmentation method)\n",
    "# run once\n",
    "# !onmt_build_vocab -config vocab_aug_1.yaml -n_sample -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# aug 1 training\n",
    "# run once\n",
    "# !onmt_train -config train_aug_1.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# aug 1 prediction\n",
    "# run once\n",
    "# !onmt_translate -model models/aug1.pt -src data/src_test.txt -output pred/aug1.txt -gpu 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate aug 1 model\n",
    "# eval('data/tgt_test.txt', 'pred/aug1.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nlp = spacy.load(\"en_core_web_trf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# threshold = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run once\n",
    "# with open('data/src_train_aug_pr.txt', 'w+') as src:\n",
    "#     with open('data/tgt_train_aug_pr.txt', 'w+') as tgt:\n",
    "#         for index, (src_1, tgt_1) in enumerate(list(zip(src_train, tgt_train))[0:-10]):\n",
    "#             for src_2, tgt_2 in list(zip(src_train, tgt_train))[index + 1: index + 11]:\n",
    "#                 if recall(set(src_1.split()), set(src_2.split())) <= threshold:\n",
    "#                     src_1_doc = nlp(src_1.strip())\n",
    "#                     src_2_doc = nlp(src_2.strip())\n",
    "#                     tgt_1_doc = nlp(tgt_1.strip())\n",
    "#                     tgt_2_doc = nlp(tgt_2.strip())\n",
    "#                     for token_1 in src_1_doc:\n",
    "#                         # print(token.text, token.pos_, token.tag_, token.dep_, [t.text for t in token.subtree])\n",
    "#                         # if ([t.text for t in token.subtree] != [t.text for t in src_1_doc]) or \\\n",
    "#                         # (len([t.text for t in token.subtree]) == 1):\n",
    "#                         #     pass\n",
    "#                         for token_2 in src_2_doc:\n",
    "#                             # print(token.text, token.pos_, token.tag_, token.dep_, [t.text for t in token.subtree])\n",
    "#                             # if ([t.text for t in token.subtree] != [t.text for t in src_2_doc]) or \\\n",
    "#                             # (len([t.text for t in token.subtree]) == 1):\n",
    "#                             #     pass\n",
    "#                             if (token_1.pos_ == token_2.pos_) and (token_1.dep_ == token_2.dep_):\n",
    "#                                 src.write(f'{src_1.replace(token_1.text, token_2.text)}')\n",
    "#                                 break\n",
    "#                         else:\n",
    "#                             continue\n",
    "#                         break\n",
    "#                     src.write(f'\\n')\n",
    "#                     for token_1 in tgt_1_doc:\n",
    "#                         # print(token.text, token.pos_, token.tag_, token.dep_, [t.text for t in token.subtree])\n",
    "#                         # if ([t.text for t in token.subtree] != [t.text for t in tgt_1_doc]) or \\\n",
    "#                         # (len([t.text for t in token.subtree]) == 1):\n",
    "#                         #     pass\n",
    "#                         for token_2 in tgt_2_doc:\n",
    "#                             # print(token.text, token.pos_, token.tag_, token.dep_, [t.text for t in token.subtree])\n",
    "#                             # if ([t.text for t in token.subtree] != [t.text for t in tgt_2_doc]) or \\\n",
    "#                             # (len([t.text for t in token.subtree]) == 1):\n",
    "#                             #     pass\n",
    "#                             if (token_1.pos_ == token_2.pos_) and (token_1.dep_ == token_2.dep_):\n",
    "#                                 tgt.write(f'{tgt_1.replace(token_1.text, token_2.text)}')\n",
    "#                                 break\n",
    "#                         else:\n",
    "#                             continue\n",
    "#                         break\n",
    "#                     tgt.write(f'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run once\n",
    "# with open('data/src_train_aug_p.txt', 'w+') as src:\n",
    "#     with open('data/tgt_train_aug_p.txt', 'w+') as tgt:\n",
    "#         for index, (src_1, tgt_1) in enumerate(list(zip(src_train, tgt_train))[0:-10]):\n",
    "#             for src_2, tgt_2 in list(zip(src_train, tgt_train))[index + 1: index + 11]:\n",
    "#                 if recall(set(src_1.split()), set(src_2.split())) <= threshold:\n",
    "#                     src_1_doc = nlp(src_1.strip())\n",
    "#                     src_2_doc = nlp(src_2.strip())\n",
    "#                     tgt_1_doc = nlp(tgt_1.strip())\n",
    "#                     tgt_2_doc = nlp(tgt_2.strip())\n",
    "#                     for token_1 in src_1_doc:\n",
    "#                         # print(token.text, token.pos_, token.tag_, token.dep_, [t.text for t in token.subtree])\n",
    "#                         # if ([t.text for t in token.subtree] != [t.text for t in src_1_doc]) or \\\n",
    "#                         # (len([t.text for t in token.subtree]) == 1):\n",
    "#                         #     pass\n",
    "#                         for token_2 in src_2_doc:\n",
    "#                             # print(token.text, token.pos_, token.tag_, token.dep_, [t.text for t in token.subtree])\n",
    "#                             # if ([t.text for t in token.subtree] != [t.text for t in src_2_doc]) or \\\n",
    "#                             # (len([t.text for t in token.subtree]) == 1):\n",
    "#                             #     pass\n",
    "#                             if token_1.pos_ == token_2.pos_:\n",
    "#                                 src.write(f'{src_1.replace(token_1.text, token_2.text)}')\n",
    "#                                 break\n",
    "#                         else:\n",
    "#                             continue\n",
    "#                         break\n",
    "#                     src.write(f'\\n')\n",
    "#                     for token_1 in tgt_1_doc:\n",
    "#                         # print(token.text, token.pos_, token.tag_, token.dep_, [t.text for t in token.subtree])\n",
    "#                         # if ([t.text for t in token.subtree] != [t.text for t in tgt_1_doc]) or \\\n",
    "#                         # (len([t.text for t in token.subtree]) == 1):\n",
    "#                         #     pass\n",
    "#                         for token_2 in tgt_2_doc:\n",
    "#                             # print(token.text, token.pos_, token.tag_, token.dep_, [t.text for t in token.subtree])\n",
    "#                             # if ([t.text for t in token.subtree] != [t.text for t in tgt_2_doc]) or \\\n",
    "#                             # (len([t.text for t in token.subtree]) == 1):\n",
    "#                             #     pass\n",
    "#                             if token_1.pos_ == token_2.pos_:\n",
    "#                                 tgt.write(f'{tgt_1.replace(token_1.text, token_2.text)}')\n",
    "#                                 break\n",
    "#                         else:\n",
    "#                             continue\n",
    "#                         break\n",
    "#                     tgt.write(f'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run once\n",
    "# with open('data/src_train_aug_r.txt', 'w+') as src:\n",
    "#     with open('data/tgt_train_aug_r.txt', 'w+') as tgt:\n",
    "#         for index, (src_1, tgt_1) in enumerate(list(zip(src_train, tgt_train))[0:-10]):\n",
    "#             for src_2, tgt_2 in list(zip(src_train, tgt_train))[index + 1: index + 11]:\n",
    "#                 if recall(set(src_1.split()), set(src_2.split())) <= threshold:\n",
    "#                     src_1_doc = nlp(src_1.strip())\n",
    "#                     src_2_doc = nlp(src_2.strip())\n",
    "#                     tgt_1_doc = nlp(tgt_1.strip())\n",
    "#                     tgt_2_doc = nlp(tgt_2.strip())\n",
    "#                     for token_1 in src_1_doc:\n",
    "#                         # print(token.text, token.pos_, token.tag_, token.dep_, [t.text for t in token.subtree])\n",
    "#                         # if ([t.text for t in token.subtree] != [t.text for t in src_1_doc]) or \\\n",
    "#                         # (len([t.text for t in token.subtree]) == 1):\n",
    "#                         #     pass\n",
    "#                         for token_2 in src_2_doc:\n",
    "#                             # print(token.text, token.pos_, token.tag_, token.dep_, [t.text for t in token.subtree])\n",
    "#                             # if ([t.text for t in token.subtree] != [t.text for t in src_2_doc]) or \\\n",
    "#                             # (len([t.text for t in token.subtree]) == 1):\n",
    "#                             #     pass\n",
    "#                             if token_1.dep_ == token_2.dep_:\n",
    "#                                 src.write(f'{src_1.replace(token_1.text, token_2.text)}')\n",
    "#                                 break\n",
    "#                         else:\n",
    "#                             continue\n",
    "#                         break\n",
    "#                     src.write(f'\\n')\n",
    "#                     for token_1 in tgt_1_doc:\n",
    "#                         # print(token.text, token.pos_, token.tag_, token.dep_, [t.text for t in token.subtree])\n",
    "#                         # if ([t.text for t in token.subtree] != [t.text for t in tgt_1_doc]) or \\\n",
    "#                         # (len([t.text for t in token.subtree]) == 1):\n",
    "#                         #     pass\n",
    "#                         for token_2 in tgt_2_doc:\n",
    "#                             # print(token.text, token.pos_, token.tag_, token.dep_, [t.text for t in token.subtree])\n",
    "#                             # if ([t.text for t in token.subtree] != [t.text for t in tgt_2_doc]) or \\\n",
    "#                             # (len([t.text for t in token.subtree]) == 1):\n",
    "#                             #     pass\n",
    "#                             if token_1.dep_ == token_2.dep_:\n",
    "#                                 tgt.write(f'{tgt_1.replace(token_1.text, token_2.text)}')\n",
    "#                                 break\n",
    "#                         else:\n",
    "#                             continue\n",
    "#                         break\n",
    "#                     tgt.write(f'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# build vocab (use PR data augmentation method)\n",
    "# run once\n",
    "# !onmt_build_vocab -config vocab_aug_pr.yaml -n_sample -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# aug pr training\n",
    "# run once\n",
    "# !onmt_train -config train_aug_pr.yaml -skip_empty_level silent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# build vocab (use P data augmentation method)\n",
    "# run once\n",
    "# !onmt_build_vocab -config vocab_aug_p.yaml -n_sample -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# aug p training\n",
    "# run once\n",
    "# !onmt_train -config train_aug_p.yaml -skip_empty_level silent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# build vocab (use R data augmentation method)\n",
    "# run once\n",
    "# !onmt_build_vocab -config vocab_aug_r.yaml -n_sample -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# aug r training\n",
    "# run once\n",
    "# !onmt_train -config train_aug_r.yaml -skip_empty_level silent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# prediction\n",
    "# run once\n",
    "# !onmt_translate -model models/augpr.pt -src data/src_test.txt -output pred/augpr.txt -gpu 0\n",
    "# !onmt_translate -model models/augp.pt -src data/src_test.txt -output pred/augp.txt -gpu 0\n",
    "# !onmt_translate -model models/augr.pt -src data/src_test.txt -output pred/augr.txt -gpu 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sent-Level Acc 0.0\n",
      "Average Precision 0.6349381434380154\n",
      "Average Recall 0.7289644432909537\n",
      "Average F1 0.6711579103481714\n",
      "New Primitive Precision 1.0\n",
      "New Primitive Recall 0.007915909680768232\n",
      "New Primitive F1 0.015707480365649545\n",
      "BLEU 0.7197529308967632\n"
     ]
    }
   ],
   "source": [
    "eval('data/tgt_test.txt', 'pred/augpr.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sent-Level Acc 0.007267064624967558\n",
      "Average Precision 0.5020979323470771\n",
      "Average Recall 0.651215503071202\n",
      "Average F1 0.5499195638113354\n",
      "New Primitive Precision 1.0\n",
      "New Primitive Recall 0.20776018686737607\n",
      "New Primitive F1 0.34404211883528524\n",
      "BLEU 0.42789970728204174\n"
     ]
    }
   ],
   "source": [
    "eval('data/tgt_test.txt', 'pred/augp.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sent-Level Acc 0.059563976122501944\n",
      "Average Precision 0.6745933904317084\n",
      "Average Recall 0.8066549874556828\n",
      "Average F1 0.7262899657657345\n",
      "New Primitive Precision 1.0\n",
      "New Primitive Recall 0.5960290682584999\n",
      "New Primitive F1 0.7468899910561835\n",
      "BLEU 0.6517496994968263\n"
     ]
    }
   ],
   "source": [
    "eval('data/tgt_test.txt', 'pred/augr.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sent-Level Acc 0.9962366986763561\n",
      "Average Precision 0.9998702309888399\n",
      "Average Recall 1.0\n",
      "Average F1 0.9999134873258932\n",
      "New Primitive Precision 1.0\n",
      "New Primitive Recall 0.9997404619776797\n",
      "New Primitive F1 0.999870214146658\n",
      "BLEU 0.9996333754237531\n"
     ]
    }
   ],
   "source": [
    "eval('data/tgt_test.txt', 'pred/aug1.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Sent-Level Acc 0.0010381520892810796\n",
      "Average Precision 0.6536248810450793\n",
      "Average Recall 0.9644432909421262\n",
      "Average F1 0.7742352897557331\n",
      "New Primitive Precision 1.0\n",
      "New Primitive Recall 0.0028549182455229693\n",
      "New Primitive F1 0.005693581780538303\n",
      "BLEU 0.7914298126967355\n"
     ]
    }
   ],
   "source": [
    "eval('data/tgt_test.txt', 'pred/base.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count(path_to_file, keyword):\n",
    "    with open(path_to_file, 'r') as f:\n",
    "        tokens = []\n",
    "        for line in f.readlines():\n",
    "            tokens += line.split()\n",
    "        print(tokens.count(keyword))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "pred/base.txt I_JUMP\n",
      "22\n",
      "pred/aug1.txt I_JUMP\n",
      "31883\n",
      "pred/augpr.txt I_JUMP\n",
      "129\n",
      "pred/augp.txt I_JUMP\n",
      "9224\n",
      "pred/augr.txt I_JUMP\n",
      "27828\n",
      "pred/base.txt I_WALK\n",
      "6924\n",
      "pred/aug1.txt I_WALK\n",
      "6543\n",
      "pred/augpr.txt I_WALK\n",
      "35449\n",
      "pred/augp.txt I_WALK\n",
      "7744\n",
      "pred/augr.txt I_WALK\n",
      "11834\n",
      "pred/base.txt I_RUN\n",
      "6640\n",
      "pred/aug1.txt I_RUN\n",
      "6584\n",
      "pred/augpr.txt I_RUN\n",
      "11377\n",
      "pred/augp.txt I_RUN\n",
      "6336\n",
      "pred/augr.txt I_RUN\n",
      "15182\n",
      "pred/base.txt I_LOOK\n",
      "6636\n",
      "pred/aug1.txt I_LOOK\n",
      "6553\n",
      "pred/augpr.txt I_LOOK\n",
      "10431\n",
      "pred/augp.txt I_LOOK\n",
      "7134\n",
      "pred/augr.txt I_LOOK\n",
      "12599\n"
     ]
    }
   ],
   "source": [
    "for keyword in ['I_JUMP', 'I_WALK', 'I_RUN', 'I_LOOK']:\n",
    "    for path in ['pred/base.txt', 'pred/aug1.txt', 'pred/augpr.txt', 'pred/augp.txt', 'pred/augr.txt']:\n",
    "        print(path, keyword)\n",
    "        count(path, keyword)"
   ]
  }
 ]
}